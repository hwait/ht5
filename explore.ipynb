{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Описание данных\n",
    "\n",
    "- `instanceId_userId` — идентификатор пользователя (анонимизированный)\n",
    "- `instanceId_objectType` — тип объекта\n",
    "- `instanceId_objectId` — идентификатор объекта (анонимизированный)\n",
    "- `feedback` — массив с типами реакций пользователя (наличие в массиве токена Liked говорит о том, что объект получил «класс» от пользователя)\n",
    "- `audit_clientType` — тип платформы, с которой зашёл пользователь\n",
    "- `audit_timestamp` — время, когда строилась лента\n",
    "- `metadata_ownerId` — автор показанного объекта (анонимизированный)\n",
    "- `metadata_createdAt` — дата создания показанного объекта\n",
    "- `audit_*` — расширенная информация о контексте построения ленты;\n",
    "- `metadata_*` — расширенная информация о самом объекте;\n",
    "- `userOwnerCounters_*` — информация о предыдущих взаимодействиях пользователя и автора контента;\n",
    "- `ownerUserCounters_*` — информация о предыдущих взаимодействиях автора контента и пользователя;\n",
    "- `membership_*` — информация о членстве пользователя в группе, где опубликован контент;\n",
    "- `user_*` — подробная информация о пользователе;\n",
    "- `auditweights_*` — большое количество runtime-признаков, извлечённых текущей системой."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.1.1`\n",
    "import $ivy.`sh.almond::almond-spark:0.11.2`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.5.2`\n",
    "\n",
    "import scala.math.Ordered._\n",
    "import scala.reflect.runtime.universe._\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession,Row}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vector}\n",
    "import org.apache.spark.ml.stat.{Correlation, Summarizer}\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,OneHotEncoder,MinMaxScaler,Normalizer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "import plotly._, plotly.element._ , plotly.layout._ , plotly.Almond._ \n",
    "\n",
    "import org.apache.log4j.{Logger, Level}\n",
    "Logger.getRootLogger.setLevel(Level.ERROR)\n",
    "Logger.getRootLogger.setLevel(Level.FATAL)\n",
    "Logger.getLogger(\"org\").setLevel(Level.WARN)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "// Data path to sna dataset\n",
    "val DATA_PATH_IN = \"../otus-bigdataml/data/sna\"\n",
    "\n",
    "// Data path where to store an intermediate data\n",
    "val DATA_PATH_OUT= \"/tmp/sna.parquet\"\n",
    "\n",
    "// Data path where to store the names of features\n",
    "val FEATURES_PATH= \"data/features.txt\"\n",
    "\n",
    "val spark=SparkSession.builder()\n",
    "    .appName(\"Data Sources Practice\")\n",
    "    .config(\"spark.master\", \"local\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val dfLoaded=spark.read.parquet(DATA_PATH_IN)\n",
    "val DF_COUNT=dfLoaded.count.toDouble\n",
    "println(s\"Data shape=(${DF_COUNT}, ${dfLoaded.columns.length})\")\n",
    "\n",
    "val dfRaw=dfLoaded\n",
    "    .withColumn(\"activity\", when(array_contains(col(\"feedback\"), \"Ignored\"), lit(0)).otherwise(lit(1))) // It is activity if user didn't ignore post, otherwise not\n",
    "    .withColumn(\"timeDelta\", ((col(\"audit_timestamp\") - col(\"metadata_createdAt\")) / 3600000 /24).cast(\"integer\"))\n",
    "    .filter(col(\"timeDelta\")>=0 && col(\"timeDelta\")<3650)\n",
    "\n",
    "def transformCategoricalValues(df:DataFrame, columns:Seq[String], labels:Array[String]):(DataFrame,Array[String]) = columns match {\n",
    "        case head::tail => {\n",
    "            val indexer = new StringIndexer()\n",
    "                .setInputCol(head)\n",
    "                .setOutputCol(head+\"_num\")\n",
    "                .setHandleInvalid(\"keep\")\n",
    "                .fit(df)\n",
    "\n",
    "            val df1=indexer.transform(df)\n",
    "\n",
    "            val encoder = new OneHotEncoder()\n",
    "                .setInputCol(head+\"_num\")\n",
    "                .setOutputCol(head+\"_vec\")\n",
    "                //.setDropLast(false)\n",
    "                .fit(df1)\n",
    "\n",
    "            transformCategoricalValues(encoder.transform(df1), tail, labels++indexer.labels.map(x=>f\"${head}_${x}\"))\n",
    "        }\n",
    "        case _ => (df.na.fill(0), labels)\n",
    "    }\n",
    "\n",
    "// It's quite voluntaristic decision to choose numerical and categorical columns \n",
    "val numericalColumns=dfRaw.columns.filter(s=>(s.startsWith(\"membership_\")&&s!=\"membership_status\")||s.startsWith(\"auditweights_\"))\n",
    "val categoricalColumns=Seq(\"instanceId_objectType\",\"audit_resourceType\",\"metadata_ownerType\",\"membership_status\")\n",
    "val (dfEnc, categoricalLabels)=transformCategoricalValues(dfRaw,categoricalColumns, Array.empty[String])\n",
    "dfEnc.createOrReplaceTempView(\"EncView\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, here we have:\n",
    "- dfEnc - dataset with one-hot encoded categorical features\n",
    "- categoricalLabels - all categorical features names\n",
    "\n",
    "It is difficult to decide the correct criteria to label churned users. I think something meaningful can be found in time gaps. \n",
    "\n",
    "So, simple calc time gaps (in days) between activities for evety user, and also the last user appearance can be important."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "// Calc time difference between user's actions in days\n",
    "val dfDateDiff = spark.sql(\"\"\"WITH DateDiff as (\n",
    "                              SELECT instanceId_userId, \n",
    "                              datediff(date, LAG(date, 1) OVER (PARTITION BY instanceId_userId ORDER BY date)) as ddiff\n",
    "                              FROM EncView)\n",
    "                              select instanceId_userId, max(ddiff) as ddiff_max from DateDiff group by instanceId_userId\n",
    "                         \"\"\")\n",
    "dfDateDiff.createOrReplaceTempView(\"DateDiffView\")\n",
    "\n",
    "// Days after the last user activity\n",
    "val dfLastActivity = spark.sql(\"\"\" select instanceId_userId, \n",
    "                                   datediff('2018-03-21', max(date)) as last_ddiff\n",
    "                                   from EncView \n",
    "                                   group by instanceId_userId\n",
    "                         \"\"\")\n",
    "dfLastActivity.createOrReplaceTempView(\"LastActivityView\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mdfDateDiff\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, ddiff_max: int]\n",
       "\u001b[36mdfLastActivity\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, last_ddiff: int]"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# User Activity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def dfBar(df:DataFrame):Bar = {\n",
    "    val (x,y) = df\n",
    "        .collect\n",
    "        .map(r=>(r(0).toString, r(1).toString.toInt))\n",
    "        .toList.unzip\n",
    "    Bar(x,y)\n",
    "}\n",
    "\n",
    "dfBar(spark.sql(\"\"\" select ddiff_max, count(ddiff_max)\n",
    "               from DateDiffView \n",
    "               where ddiff_max is not null\n",
    "               group by ddiff_max\n",
    "               order by ddiff_max\n",
    "        \"\"\")).plot(title = \"Activity date diff\")\n",
    "\n",
    "dfBar(spark.sql(\"\"\" select last_ddiff, count(last_ddiff)\n",
    "               from LastActivityView \n",
    "               where last_ddiff is not null\n",
    "               group by last_ddiff\n",
    "               order by last_ddiff\n",
    "        \"\"\")).plot(title = \"Last Activity date diff\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1097138054\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\"],\"y\":[55081.0,54390.0,54625.0,59795.0,68941.0,79003.0,93436.0,105981.0,115956.0,118599.0,121612.0,124648.0,125627.0,120531.0,116022.0,108584.0,101087.0,93433.0,87884.0,81124.0,75864.0,69337.0,61920.0,56084.0,51561.0,45499.0,41344.0,37625.0,34975.0,30728.0,27218.0,24457.0,22624.0,20865.0,18461.0,16405.0,14328.0,12370.0,10725.0,10240.0,8955.0,7612.0,6559.0,5176.0,4119.0,3171.0,2226.0,1388.0,626.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"Activity date diff\"};\n",
       "\n",
       "  Plotly.plot('plot-1097138054', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ],
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "x": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "38",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48"
         ],
         "y": [
          55081,
          54390,
          54625,
          59795,
          68941,
          79003,
          93436,
          105981,
          115956,
          118599,
          121612,
          124648,
          125627,
          120531,
          116022,
          108584,
          101087,
          93433,
          87884,
          81124,
          75864,
          69337,
          61920,
          56084,
          51561,
          45499,
          41344,
          37625,
          34975,
          30728,
          27218,
          24457,
          22624,
          20865,
          18461,
          16405,
          14328,
          12370,
          10725,
          10240,
          8955,
          7612,
          6559,
          5176,
          4119,
          3171,
          2226,
          1388,
          626
         ],
         "type": "bar"
        }
       ],
       "layout": {
        "title": "Activity date diff"
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-2146386214\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\"],\"y\":[290442.0,241420.0,214390.0,199731.0,167049.0,145655.0,137774.0,130195.0,120720.0,115347.0,119369.0,107366.0,100855.0,91250.0,86804.0,82644.0,80280.0,83380.0,73998.0,70386.0,65695.0,60273.0,59054.0,58838.0,64969.0,58337.0,54607.0,53802.0,51399.0,50175.0,42578.0,37897.0,29534.0,45278.0,45396.0,43272.0,40889.0,41432.0,40428.0,38267.0,37655.0,36712.0,36444.0,35600.0,37234.0,32356.0,29993.0,29124.0],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"Last Activity date diff\"};\n",
       "\n",
       "  Plotly.plot('plot-2146386214', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ],
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "x": [
          "0",
          "1",
          "2",
          "3",
          "4",
          "5",
          "6",
          "7",
          "8",
          "9",
          "10",
          "11",
          "12",
          "13",
          "14",
          "15",
          "16",
          "17",
          "18",
          "19",
          "20",
          "21",
          "22",
          "23",
          "24",
          "25",
          "26",
          "27",
          "28",
          "29",
          "30",
          "31",
          "32",
          "33",
          "34",
          "35",
          "36",
          "37",
          "39",
          "40",
          "41",
          "42",
          "43",
          "44",
          "45",
          "46",
          "47",
          "48"
         ],
         "y": [
          290442,
          241420,
          214390,
          199731,
          167049,
          145655,
          137774,
          130195,
          120720,
          115347,
          119369,
          107366,
          100855,
          91250,
          86804,
          82644,
          80280,
          83380,
          73998,
          70386,
          65695,
          60273,
          59054,
          58838,
          64969,
          58337,
          54607,
          53802,
          51399,
          50175,
          42578,
          37897,
          29534,
          45278,
          45396,
          43272,
          40889,
          41432,
          40428,
          38267,
          37655,
          36712,
          36444,
          35600,
          37234,
          32356,
          29993,
          29124
         ],
         "type": "bar"
        }
       ],
       "layout": {
        "title": "Last Activity date diff"
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mdfBar\u001b[39m\n",
       "\u001b[36mres12_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1097138054\"\u001b[39m\n",
       "\u001b[36mres12_2\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-2146386214\"\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### My intuition here tells abouth 23th day as a key point. If user has no activity 23+ days I suppose there is high probability he/she/it will never return."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "val dfLabeled = spark.sql(\"\"\"SELECT instanceId_userId, case when last_ddiff >= 23 then 1 else 0 end as label\n",
    "                              FROM LastActivityView                              \n",
    "                         \"\"\")\n",
    "dfLabeled.createOrReplaceTempView(\"LastActivityView\")\n",
    "//println(f\" There is ${spark.sql(\"select sum(label)/count(label) from LastActivityView\").first()(0).toString.toDouble*100}%.1f%% of users are churned\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mdfLabeled\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, label: int]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare Features\n",
    "I suppose for the task not necessary to make complicated futures engineering, time windows, dynamics etc. \n",
    "As a features I calculated for every user:\n",
    "- means of numerical columns;\n",
    "- sum and maean of activities (if user didn't ignore post it counts as 1);\n",
    "- max of time difference between user's activities in days.\n",
    "\n",
    "It is free of wasting because labeling was based on last activity lag, instead of inner ddiff_max. \n",
    "\n",
    "All Data is scaled with MinMaxScaler. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "val summarizer = Summarizer.metrics(\"mean\")\n",
    "val assemblerCol=\"assemblerCol\"\n",
    "val assembler = new VectorAssembler()\n",
    "    .setInputCols(categoricalColumns.map(_+\"_vec\").toArray++numericalColumns)\n",
    "    .setOutputCol(assemblerCol) \n",
    "    \n",
    "val dfAggregated=assembler\n",
    "                    .transform(dfEnc)\n",
    "                    .groupBy($\"instanceId_userId\")\n",
    "                    .agg(\n",
    "                        Summarizer.mean(col(assemblerCol)).alias(\"features_mean\"),\n",
    "                        mean($\"activity\").alias(\"activity_mean\"),\n",
    "                        sum($\"activity\").alias(\"activity_sum\")\n",
    "                    )\n",
    "\n",
    "dfAggregated.createOrReplaceTempView(\"AggregatedView\")\n",
    "\n",
    "val dfCombined = spark.sql(\"\"\" SELECT a.instanceId_userId,\n",
    "                                   a.features_mean, \n",
    "                                   a.activity_mean,\n",
    "                                   a.activity_sum,\n",
    "                                   b.ddiff_max,\n",
    "                                   c.label\n",
    "                              FROM AggregatedView a\n",
    "                              JOIN DateDiffView b on a.instanceId_userId = b.instanceId_userId\n",
    "                              JOIN LastActivityView c on a.instanceId_userId = c.instanceId_userId\n",
    "                         \"\"\").na.drop()\n",
    "                         \n",
    "dfCombined.createOrReplaceTempView(\"CombinedView\")\n",
    "\n",
    "val featuresNames=categoricalLabels++numericalColumns:+\"activity_mean\":+\"activity_sum\":+\"ddiff_max\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36msummarizer\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mSummaryBuilder\u001b[39m = org.apache.spark.ml.stat.SummaryBuilderImpl@5bff98a9\n",
       "\u001b[36massemblerCol\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"assemblerCol\"\u001b[39m\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = VectorAssembler: uid=vecAssembler_0e283d1baf06, handleInvalid=error, numInputCols=65\n",
       "\u001b[36mdfAggregated\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 2 more fields]\n",
       "\u001b[36mdfCombined\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 4 more fields]\n",
       "\u001b[36mfeaturesNames\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"instanceId_objectType_Post\"\u001b[39m,\n",
       "  \u001b[32m\"instanceId_objectType_Photo\"\u001b[39m,\n",
       "  \u001b[32m\"instanceId_objectType_Video\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_8\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_3\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_7\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_6\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_14\"\u001b[39m,\n",
       "  \u001b[32m\"metadata_ownerType_GROUP_OPEN_OFFICIAL\"\u001b[39m,\n",
       "  \u001b[32m\"metadata_ownerType_GROUP_OPEN\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_A\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_P\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_I\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_M\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_Y\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_!\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_B\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_R\"\u001b[39m,\n",
       "  \u001b[32m\"membership_statusUpdateDate\"\u001b[39m,\n",
       "  \u001b[32m\"membership_joinDate\"\u001b[39m,\n",
       "  \u001b[32m\"membership_joinRequestDate\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ageMs\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_closed\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ctr_gender\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ctr_high\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ctr_negative\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_dailyRecency\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_feedOwner_RECOMMENDED_GROUP\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_feedStats\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendCommentFeeds\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendCommenters\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendLikes\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendLikes_actors\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_hasDetectedText\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_hasText\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_isPymk\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_isRandom\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_likersFeedStats_hyper\"\u001b[39m,\n",
       "..."
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "val FEATURES_PATH= \"data/features.txt\"\n",
    "spark.sparkContext.parallelize(featuresNames).saveAsObjectFile(FEATURES_PATH)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mFEATURES_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/features.txt\"\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "val pipeline = new Pipeline().setStages(Array(\n",
    "    new VectorAssembler()\n",
    "        .setInputCols(Array(\"features_mean\",\"activity_mean\",\"activity_sum\",\"ddiff_max\"))\n",
    "        .setOutputCol(\"features_comb\"),\n",
    "    //new MinMaxScaler().setInputCol(\"features_comb\").setOutputCol(\"features\")\n",
    "    ))\n",
    "\n",
    "val dfAssembled=pipeline.fit(dfCombined).transform(dfCombined)\n",
    "dfAssembled.write.mode(\"overwrite\").parquet(DATA_PATH_OUT)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mpipeline\u001b[39m: \u001b[32mPipeline\u001b[39m = pipeline_87b7817ed398\n",
       "\u001b[36mdfAssembled\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 5 more fields]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "name": "scala",
   "version": "2.12.9",
   "mimetype": "text/x-scala",
   "file_extension": ".sc",
   "nbconvert_exporter": "script",
   "codemirror_mode": "text/x-scala"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}