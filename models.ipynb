{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.1`\n",
    "import $ivy.`org.apache.spark::spark-mllib:3.1.1`\n",
    "import $ivy.`sh.almond::almond-spark:0.11.2`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.5.2`\n",
    "\n",
    "import java.nio.file.{Paths, Files}\n",
    "import scala.math.Ordered._\n",
    "import scala.reflect.runtime.universe._\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession,Row}\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.linalg.{Matrix, Vector}\n",
    "import org.apache.spark.ml.stat.{Correlation, Summarizer}\n",
    "import org.apache.spark.ml.feature.{VectorAssembler,StringIndexer,OneHotEncoder,MinMaxScaler,Normalizer}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "import plotly._, plotly.element._ , plotly.layout._ , plotly.Almond._ \n",
    "\n",
    "import org.apache.log4j.{Logger, Level}\n",
    "Logger.getRootLogger.setLevel(Level.ERROR)\n",
    "Logger.getRootLogger.setLevel(Level.FATAL)\n",
    "Logger.getLogger(\"org\").setLevel(Level.WARN)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.nio.file.{Paths, Files}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.math.Ordered._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.reflect.runtime.universe._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.{DataFrame, SparkSession,Row}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions.Window\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.linalg.{Matrix, Vector}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.stat.{Correlation, Summarizer}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.{VectorAssembler,StringIndexer,OneHotEncoder,MinMaxScaler,Normalizer}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.Pipeline\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._, plotly.element._ , plotly.layout._ , plotly.Almond._ \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Logger, Level}\n",
       "\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "val DATA_PATH_IN= \"/tmp/sna.parquet\"\n",
    "val FEATURES_PATH= \"data/features.txt\"\n",
    "\n",
    "println(\"Checking necessary files:\")\n",
    "if (Files.exists(Paths.get(FEATURES_PATH))) println(\"Found features\") else println(f\"ERROR! There is no data found on ${FEATURES_PATH}! Please run explore.ipynb first!\") \n",
    "if (Files.exists(Paths.get(DATA_PATH_IN))) println(\"Found data source\") else println(f\"ERROR! There is no data found on ${DATA_PATH_IN}! Please run explore.ipynb first!\") "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking necessary files:\n",
      "Found features\n",
      "Found data source\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mDATA_PATH_IN\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/tmp/sna.parquet\"\u001b[39m\n",
       "\u001b[36mFEATURES_PATH\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/features.txt\"\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "val spark=SparkSession.builder()\n",
    "    .appName(\"Data Sources Practice\")\n",
    "    .config(\"spark.master\", \"local\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "val featuresNames=spark.sparkContext.objectFile[String](FEATURES_PATH).collect\n",
    "val dfAssembled=spark.read.parquet(DATA_PATH_IN)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/08/10 19:46:55 WARN Utils: Your hostname, DESKTOP-G76NQH1 resolves to a loopback address: 127.0.1.1; using 172.17.97.97 instead (on interface eth0)\n",
      "21/08/10 19:46:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/10 19:46:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@17406859\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mfeaturesNames\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"instanceId_objectType_Post\"\u001b[39m,\n",
       "  \u001b[32m\"instanceId_objectType_Photo\"\u001b[39m,\n",
       "  \u001b[32m\"instanceId_objectType_Video\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_8\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_3\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_7\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_6\"\u001b[39m,\n",
       "  \u001b[32m\"audit_resourceType_14\"\u001b[39m,\n",
       "  \u001b[32m\"metadata_ownerType_GROUP_OPEN_OFFICIAL\"\u001b[39m,\n",
       "  \u001b[32m\"metadata_ownerType_GROUP_OPEN\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_A\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_P\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_I\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_M\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_Y\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_!\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_B\"\u001b[39m,\n",
       "  \u001b[32m\"membership_status_R\"\u001b[39m,\n",
       "  \u001b[32m\"membership_statusUpdateDate\"\u001b[39m,\n",
       "  \u001b[32m\"membership_joinDate\"\u001b[39m,\n",
       "  \u001b[32m\"membership_joinRequestDate\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ageMs\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_closed\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ctr_gender\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ctr_high\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_ctr_negative\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_dailyRecency\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_feedOwner_RECOMMENDED_GROUP\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_feedStats\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendCommentFeeds\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendCommenters\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendLikes\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_friendLikes_actors\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_hasDetectedText\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_hasText\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_isPymk\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_isRandom\"\u001b[39m,\n",
       "  \u001b[32m\"auditweights_likersFeedStats_hyper\"\u001b[39m,\n",
       "...\n",
       "\u001b[36mdfAssembled\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 5 more fields]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "val splits:Array[DataFrame] = dfAssembled\n",
    "    //.select($\"features\",$\"label\")\n",
    "    .randomSplit(Array(0.7, 0.3), seed = 21)\n",
    "\n",
    "val scaler=new MinMaxScaler().setInputCol(\"features_comb\").setOutputCol(\"features\").fit(splits(0))\n",
    "val dfTrain=scaler.transform(splits(0))\n",
    "val dfTest=scaler.transform(splits(1))\n",
    "\n",
    "val label=\"label\"\n",
    "val features=\"features\""
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36msplits\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDataFrame\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [instanceId_userId: int, features_mean: vector ... 5 more fields],\n",
       "  [instanceId_userId: int, features_mean: vector ... 5 more fields]\n",
       ")\n",
       "\u001b[36mscaler\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mMinMaxScalerModel\u001b[39m = MinMaxScalerModel: uid=minMaxScal_319eee0e56f5, numFeatures=82, min=0.0, max=1.0\n",
       "\u001b[36mdfTrain\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 6 more fields]\n",
       "\u001b[36mdfTest\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 6 more fields]\n",
       "\u001b[36mlabel\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"label\"\u001b[39m\n",
       "\u001b[36mfeatures\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"features\"\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models\n",
    "## RandomForestClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(label)\n",
    "  .setFeaturesCol(features)\n",
    "  .setNumTrees(20)\n",
    "\n",
    "val rfModel = rf.fit(dfTrain)\n",
    "val dfrfOutput = rfModel.transform(dfTest)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.RandomForestClassifier\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrf\u001b[39m: \u001b[32mRandomForestClassifier\u001b[39m = rfc_83c1b29327ff\n",
       "\u001b[36mrfModel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mRandomForestClassificationModel\u001b[39m = RandomForestClassificationModel: uid=rfc_83c1b29327ff, numTrees=20, numClasses=2, numFeatures=81\n",
       "\u001b[36mdfrfOutput\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 9 more fields]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NaiveBayes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import org.apache.spark.ml.classification.NaiveBayes\n",
    "\n",
    "val nb = new NaiveBayes()\n",
    "  .setLabelCol(label)\n",
    "  .setFeaturesCol(features)\n",
    "\n",
    "val nbModel = nb.fit(dfTrain)\n",
    "val dfnbOutput = nbModel.transform(dfTest)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.NaiveBayes\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mnb\u001b[39m: \u001b[32mNaiveBayes\u001b[39m = nb_43e4612db691\n",
       "\u001b[36mnbModel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mNaiveBayesModel\u001b[39m = NaiveBayesModel: uid=nb_43e4612db691, modelType=multinomial, numClasses=2, numFeatures=81\n",
       "\u001b[36mdfnbOutput\u001b[39m: \u001b[32mDataFrame\u001b[39m = [instanceId_userId: int, features_mean: vector ... 9 more fields]"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GBTClassifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import org.apache.spark.ml.classification.GBTClassifier\n",
    "\n",
    "val gbt = new GBTClassifier()\n",
    "  .setLabelCol(label)\n",
    "  .setFeaturesCol(features)\n",
    "  .setMaxIter(20)\n",
    "\n",
    "val gbtModel = gbt.fit(dfTrain)\n",
    "val dfGbtOutput = gbtModel.transform(dfTest).cache()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.GBTClassifier\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mgbt\u001b[39m: \u001b[32mGBTClassifier\u001b[39m = gbtc_f04ac639e166\n",
       "\u001b[36mgbtModel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mGBTClassificationModel\u001b[39m = GBTClassificationModel: uid = gbtc_f04ac639e166, numTrees=20, numClasses=2, numFeatures=82\n",
       "\u001b[36mdfGbtOutput\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [instanceId_userId: int, features_mean: vector ... 9 more fields]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def metrics(name:String, df:DataFrame) = {\n",
    "    val res=df.groupBy(\"label\", \"prediction\")\n",
    "        .agg(count(\"prediction\").as(\"total\"))\n",
    "        .select($\"total\").as[Long].collect\n",
    "    val Array(fn,tn,tp,fp) = res.map(_.toDouble)\n",
    "    val precision = tp / (tp + fp) // Precision (Positive Predictive Value)\n",
    "    val recall = tp / (tp + fn) // Recall (True Positive Rate)\n",
    "    val f1 = (2 * precision * recall) / (precision + recall)\n",
    "    f\"${name}: PRECISION=${precision}%.2f, RECALL=${recall}%.2f, F1 score=${f1}%.2f\"\n",
    "}\n",
    "\n",
    "println(metrics(\"NB\", dfnbOutput))\n",
    "println(metrics(\"RF\", dfrfOutput))\n",
    "println(metrics(\"GBT\", dfGbtOutput))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/10 02:54:39 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/08/10 02:54:39 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NB: PRECISION=0.95, RECALL=0.00, F1 score=0.00\n",
      "RF: PRECISION=0.89, RECALL=0.26, F1 score=0.40\n",
      "GBT: PRECISION=0.85, RECALL=0.70, F1 score=0.77\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmetrics\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best model is GBTClassifier, let's use it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import org.apache.spark.ml.classification.GBTClassificationModel\n",
    "val importances = gbtModel // this would be your trained model\n",
    "  .asInstanceOf[GBTClassificationModel]\n",
    "  .featureImportances.toSparse\n",
    "val featureImportances = ((for (idx<-importances.indices) yield (featuresNames(idx))) zip importances.values).sortWith(_._2 > _._2).take(15)\n",
    "val (x,y) = featureImportances.unzip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "Bar(x.toList,y.toList).plot(title = \"Features Importance\")"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min',\n",
       "    jquery: 'https://code.jquery.com/jquery-3.3.1.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-2094044100\"></div>\n",
       "<script>require(['plotly'], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"x\":[\"auditweights_svd_spark\",\"ddiff_max\",\"activity_mean\",\"activity_sum\",\"auditweights_svd_prelaunch\",\"audit_resourceType_8\",\"auditweights_userOwner_IMAGE\",\"auditweights_userOwner_CREATE_LIKE\",\"auditweights_likersSvd_prelaunch_hyper\",\"auditweights_dailyRecency\",\"metadata_ownerType_GROUP_OPEN_OFFICIAL\",\"audit_resourceType_7\",\"auditweights_ageMs\",\"auditweights_feedOwner_RECOMMENDED_GROUP\",\"auditweights_userOwner_TEXT\"],\"y\":[0.44478713249373947,0.23709034845474655,0.08954392201039846,0.0828841992398071,0.05393175369237726,0.014181614080082144,0.013811549127821808,0.011556558101794033,0.010459961808377239,0.00920839493992237,0.008657972699699711,0.006617198148419236,0.006046900445956903,0.0039301037246992345,0.003343636906987331],\"type\":\"bar\"};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"Features Importance\"};\n",
       "\n",
       "  Plotly.plot('plot-2094044100', data, layout);\n",
       "})();\n",
       "});\n",
       "      </script>\n",
       "           "
      ],
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "x": [
          "auditweights_svd_spark",
          "ddiff_max",
          "activity_mean",
          "activity_sum",
          "auditweights_svd_prelaunch",
          "audit_resourceType_8",
          "auditweights_userOwner_IMAGE",
          "auditweights_userOwner_CREATE_LIKE",
          "auditweights_likersSvd_prelaunch_hyper",
          "auditweights_dailyRecency",
          "metadata_ownerType_GROUP_OPEN_OFFICIAL",
          "audit_resourceType_7",
          "auditweights_ageMs",
          "auditweights_feedOwner_RECOMMENDED_GROUP",
          "auditweights_userOwner_TEXT"
         ],
         "y": [
          0.44478713249373947,
          0.23709034845474655,
          0.08954392201039846,
          0.0828841992398071,
          0.05393175369237726,
          0.014181614080082144,
          0.013811549127821808,
          0.011556558101794033,
          0.010459961808377239,
          0.00920839493992237,
          0.008657972699699711,
          0.006617198148419236,
          0.006046900445956903,
          0.0039301037246992345,
          0.003343636906987331
         ],
         "type": "bar"
        }
       ],
       "layout": {
        "title": "Features Importance"
       }
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-2094044100\"\u001b[39m"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Well, the best feature is one from numerical features, I suppose it derived from auditweight analyse.  But next three places taken by newly designed features. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plotBox(df:DataFrame, feature:String, title:String) = {\n",
    "    val (x,y) = df.select($\"label\",col(feature))\n",
    "        .collect\n",
    "        .map(r=>(r(0).toString, r(1).toString.toDouble))\n",
    "        .toList.unzip\n",
    "    Box(x,y,orientation = Orientation.Horizontal).plot(title = title)\n",
    "}\n",
    "plotBox(dfGbtOutput, \"ddiff_max\", \"Distribution of ddiff_max\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is a lot of data by plotly, so I've saved diagrams to reduce notebook size.\n",
    "\n",
    "![](./images/ddiff_max.png)\n",
    "\n",
    "ddiff_max with median 7 anf 15 for both classes accordingly, clearly shows significant difference between classes. Churned users (class 1) have more then twice less time difference between activities. It is counterintuitive and go against common sense. It should be vice versa, and large laps in user activity must be consistent with greater chance user to churn. \n",
    "\n",
    "I have only one explanation - we have observations for a short period of time, so large gaps inside that period corresponds to small last gap (less then 23 days), so user counts as not churned, but who knows what has happen later. \n",
    "\n",
    "So, we have to make analysis with uncertain target and unclear criteries.  The probability of getting an appropriate result is vanishingly small."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plotBox(dfGbtOutput, \"activity_sum\", \"Distribution of activity_sum\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./images/activity_sum.png)\n",
    "\n",
    "Churned users are much less active, so this feature make sense!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plotBox(dfGbtOutput, \"activity_mean\", \"Distribution of activity_mean\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](./images/activity_mean.png)\n",
    "\n",
    "No much difference between groups here, so I can't make any meaningful conclusions why this parameter so important for the model."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "name": "scala",
   "version": "2.12.9",
   "mimetype": "text/x-scala",
   "file_extension": ".sc",
   "nbconvert_exporter": "script",
   "codemirror_mode": "text/x-scala"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}